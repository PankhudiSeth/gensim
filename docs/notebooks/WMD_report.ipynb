{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Run for table of contents.\n",
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
       "\n",
       "// https://github.com/kmahelona/ipython_notebook_goodies"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Run for table of contents.\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "\n",
    "// https://github.com/kmahelona/ipython_notebook_goodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMD implementation in Gensim\n",
    "\n",
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>\n",
    "\n",
    "Note: run JavaScript cell in beginning to get table of contents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report covers the implementation of WMD (Word Mover's Distance) in Gensim. It is being worked on at my [word_movers_distance](https://github.com/olavurmortensen/gensim/tree/word_movers_distance) branch of Gensim, and has a pull-request at [#521](https://github.com/piskvorky/gensim/pull/521).\n",
    "\n",
    "The WMD tutorial on my Gensim WMD branch can be found [here](https://github.com/olavurmortensen/gensim/blob/word_movers_distance/docs/notebooks/WMD_tutorial.ipynb). **Read this tutorial to get an understanding of how WMD works and how to use it in Gensim.**\n",
    "\n",
    "To understand the model properly, it is also necessary to read the paper, written by Matt J. Kusner, titled [\"From Word Embeddings To Document Distances\"](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf).\n",
    "\n",
    "This pull-request:\n",
    "* adds one function to `gensim.models.word2vec`, called `wmdistance`.\n",
    "* adds a class to `gensim.similarities.docsim` called `WmdSimilarity`.\n",
    "\n",
    "Unit tests of `wmdistance` are added to `gensim.test.test_word2vec`, and unit tests of `WmdSimilarity` are added to `gensim.test.test_similarities`.\n",
    "\n",
    "A release of WMD has been worked on at the pull-request [#619](https://github.com/piskvorky/gensim/pull/619). #619 is missing some features that are in #521, which are not ready yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyemd\n",
    "\n",
    "To compute the WMD distance, we use a Python wrapper of a C++ implementation of EMD (Earth Mover's Distance) called [pyemd](https://github.com/wmayner/pyemd). \n",
    "\n",
    "In `gensim.models.word2vec`, a flag `PYEMD_EXT` flag is set to `True` if `emd` is successfully imported from `pemd`. Otherwise, it sets it to `False`, and if WMD is attempted to be used an error is raised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The wmdistance function\n",
    "\n",
    "First, all words that are not in the Word2Vec model's vocabulary are removed, as we cannot compute the distances to such words.\n",
    "\n",
    "If any of the documents are empty after OOV word removal, we cannot compute the distance. In this case `float('inf')` is returned.\n",
    "\n",
    "We make a dictionary of the documents using the `gensim.corpora.Dictionary` object.\n",
    "\n",
    "### Distance matrix\n",
    "\n",
    "To compute the WMD distance, we first compute the distance between the words in each input document, i.e. the distance matrix. The euclidean distance between the word2vec embeddings of the words in the documents are used.\n",
    "\n",
    "Only the distances that we need are computed, i.e. distances from document 1 to document 2. All other distances are zero.\n",
    "\n",
    "### nBOW\n",
    "\n",
    "nBOW is the normalize bag-of-words representation. We compute the frequency of each word in the document, and normalize these frequencies by the length of the document. The indeces in the nBOW array correspond to the words in the `gensim.corpora.Dictionary` object.\n",
    "\n",
    "### Computing the WMD\n",
    "\n",
    "To compute the WMD, we simply pass the nBOW representation of the documents and the distance matrix to `pyemd.emd`, which solves the optimization problem for us.\n",
    "\n",
    "### WCD\n",
    "\n",
    "We do *not* need the distance matrix to compute the WCD distance. We do however need the nBOW vectors. To compute the WCD, we stack all the word embeddings (i.e. the Word2Vec vectors) in a matrix $X$, and compute $||X d_1 - X d_2||_2$, where $d_1$ and $d_2$ are the nBOW vectors.\n",
    "\n",
    "### RWMD\n",
    "\n",
    "To compute RWMD, we *do* need the distance matrix. We compute the minimal values in the distance matrix, row-wise and column-wise. We then dot those minimum vectors with the nBOW vectors, getting two scalars $RWMD_1$ and $RWMD_2$. We then return $RWMD = \\max(RWMD_1, RWMD_2)$.\n",
    "\n",
    "When we compute RWMD, the indeces in the distance matrix that correspond to pairs of words that do not exist are set to NaN. This is done so that we can use `numpy.nanmin` to find the minimal values that do *not* correspond to word pairs that we do not have in the input doduments. In other words, if we do not do this, the minimum of the distance matrix just returns zeros. There are other ways of dealing with this (for example using \"masked arrays\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The WmdSimilarity class\n",
    "\n",
    "The `WmdSimilarity` class extends the `SimilarityABC` interface in Gensim. It might be necessary to understand this interface to fully understand the `WmdSimilarity` implementation.\n",
    "\n",
    "For `WmdSimilarity` to work as a similarity class, where we can use the syntax:\n",
    "```\n",
    "# Given a document collection \"corpus\", train word2vec model.\n",
    "model = word2vec(corpus)\n",
    "instance = WmdSimilarity(corpus, model, num_best=10)\n",
    "\n",
    "# Make query.\n",
    "sims = instance[query]\n",
    "```\n",
    "\n",
    "To be able to do this, it needs to implement a function `get_similarities` that takes a \"query\", calculates the similarity between the query and each document in the corpus, and returns a vector of similarities.\n",
    "\n",
    "`get_similarities` also needs to handle a list of queries. \n",
    "\n",
    "### Prefetch and prune\n",
    "\n",
    "if `pp = True`, the prefetch and prune algorithm is used. At the moment, the prefetch and prune algorithm is rather long and confusing, perhaps this can be fixed at some point.\n",
    "\n",
    "The algorithm works as in the paper [1], that is,\n",
    "\n",
    "* Compute WCD distance to all documents in corpus.\n",
    "* Sort according to WCD.\n",
    "* Compute WMD distances of the first $k$ documents.\n",
    "* For the rest of the $N - k$ documents:\n",
    "    * Compute RWMD of current document.\n",
    "    * If RWMD is greater than WMD of the $k$th document, *continue* (i.e. \"prune\").\n",
    "    * Else, compute WMD of current document.\n",
    "        * If it is greater than WMD of the $k$th document, *continue*.\n",
    "        * Else, add it to the $k$ nearest documents.\n",
    "\n",
    "However, there are some caveats.\n",
    "\n",
    "We are not interested in documents that are identical to the input document. This is how the `SimilarityABC` interface works when `num_best` is not `None`, and it is never `None` when prefetch and prune is being used. To avoid this problem we do two things: (1) if WCD is zero at the start of the algorithm, set it to `float('inf')`, (2) if WMD of a document is zero (during prune process), ignore it and continue.\n",
    "\n",
    "The `SimilarityABC` interface expects a list of similarities between the query and *all* the documents in the corpus, no just the $k$th (i.e. `num_best`) most similar. So we simply set the closest to their actual WMD distances, and the rest to some large number, such that they are ignored in the sorting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit tests\n",
    "\n",
    "### Testing wmdistance\n",
    "\n",
    "The unit tests for `wmdistance` are in `gensim.test.test_word2vec`. A Word2Vec model is trained on a very small corpus defined in the start of `test_word2vec`. `min_count=1` is used so that the model contains some words even though the corpus is small. `seed=42` and `workers=1` is used so that the results don't change each time you run the test (although it may be different on different machines).\n",
    "\n",
    "The following tests are in place:\n",
    "\n",
    "* testNonzero:\n",
    "    * Compute the WMD distance between some sentences and check that the distance is not zero. This is basically to test that WMD is working. It is not a good idea to test that the distance equals some specific value, as the result may vary.\n",
    "* testSymmetry:\n",
    "    * Test that the the distance between sentence 1 and 2 is the same as between sentence 2 and 1.\n",
    "* testIdenticalSentences:\n",
    "    * Test that the distance between a sentence and itself is zero.\n",
    "* WCD:\n",
    "    * All the same tests as above for WCD.\n",
    "* RWMD:\n",
    "    * All the same tests as above for RWMD.\n",
    "* testOrderWCD:\n",
    "    * Check that WCD is less than WMD, as it is defined to be a lower bound.\n",
    "* testOrderRWMD:\n",
    "    * Check that RWMD is less than WMD, as it is defined to be a lower bound.\n",
    "\n",
    "Note that it does not necessarily hold that WCD is less than RWMD, although it is the case a large portion of the time.\n",
    "\n",
    "### Testing WmdSimilarity\n",
    "\n",
    "There are tests in place in `gensim.test.test_similarities` that test all the similarity classes. Some of these, however, do not work for `WmdSimilarity`. These are therefore overwritten.\n",
    "\n",
    "There is one test added to `WmdSimilarity` that isn't tested for the other similarity classes. This is the `testNonIncreasing` test, which (as the name suggests) checks that the similarity vector is non-increasing. It does not need to be strictly decreasing.\n",
    "\n",
    "The tests of `WmdSimilarity` are implemented in a way very similar to the other similarity class tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefetch and prune\n",
    "\n",
    "The prefetch and prune algorithm is supposed to speed up the corpus based similarity computation. At the moment, it is not faster, which is why it is excluded from the first release of WMD.\n",
    "\n",
    "Most likely, the primary problem is that the Python code that computes the distance matrix, RWMD and WCD is too slow in comparison to the fast C++ code that computes WMD. Therefore, what will most likely give the most speed-up is to convert as much as possible of the `wmdistance` code to **Cython** code.\n",
    "\n",
    "Another problem is that I didn't experience the level of pruning claimed in the paper [1]. Pruning means discarding a document as a potential candidate without having to compute the full WMD distance. According to the paper, for some data sets they are able to prune up to 95%. In the data set I tested I experienced about 55% pruning.\n",
    "\n",
    "### Pre-computing distances\n",
    "\n",
    "In an attempt to mend this problem, I have tried to precompute all the distances in the vocabulary. The distances are computed in `gensim.models.word2vec.init_distances`. This makes prefetch and prune significantly faster.\n",
    "\n",
    "A better way to do this is by *memoization*, computing distances between pairs of words when they are seen for the first time, and adding them to the pre-computed distance dictionary.\n",
    "\n",
    "Pre-computing the distances can take up a lot of memory, use with caution.\n",
    "\n",
    "As mentioned in the WMD tutorial [3], `gensim.models.word2vec.init_sims` is used used to normalize the word embeddings. `init_sims` must be called *before* `init_distances`. If the distances are pre-computed, and then the word vectors are normalized, the distances will be wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure Python WMD\n",
    "\n",
    "As mentioned earlier, this WMD implementation relies on a C++ implementation of EMD (through the `pyemd` Python wrapper). \n",
    "\n",
    "It was considered in the beginning to make a brute force algorithm, purely in Python. It turned out not to be possible to make such a \"brute force\" algorithm; to compute the WMD distance, you have to solve the *transportation problem* (explained in [1]).\n",
    "\n",
    "The transportation problem can be formulated as a linear program, and as such it should be possible to solve it using SciPy's linear programming solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Matt J. Kusner et al., *From Word Embeddings To Document Distances*, 2015.\n",
    "* WMD Gensim branch, https://github.com/olavurmortensen/gensim/tree/word_movers_distance.\n",
    "* WMD Gensim tutorial, https://github.com/olavurmortensen/gensim/blob/word_movers_distance/docs/notebooks/WMD_tutorial.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
