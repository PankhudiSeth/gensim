{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Mover's Distance\n",
    "\n",
    "Word Mover's Distance (WMD) is a new method that seeks to find the distance between two sets of words (documents, sentences, etc.). The method uses word embeddings via word2vec, and the rest is quite intuitive: match the closest words and sum the distances between them. Another way of saying it is that we find the \"minimum traveling distance\" from one document to another. \n",
    "\n",
    "WMD is illustrated below for two very similar sentences. The sentences have no words in common (non-trivial ones), but by matching the relevant words, WMD is able to accurately measure the similarity between the two sentences.\n",
    "\n",
    "<img src='https://vene.ro/images/wmd-obama.png' height='700' width='700'>\n",
    "\n",
    "\n",
    "This method comes from the article \"From Word Embeddings To Document Distances\" by Matt Kusner et al. ([link to PDF](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)). It is inspired by the \"Earth Mover's Distance\", and employs a solver of the \"transportation problem\".\n",
    "\n",
    "This short tutorial shows the use of the `wmdistance` method of the Gensim `Word2Vec` class. Stay tuned for another tutorial in kNN classification using Gensim's [docsim](http://radimrehurek.com/gensim/similarities/docsim.html), when this functionality is implemented!\n",
    "\n",
    "## Using WMD\n",
    "\n",
    "To use WMD, we need some word embeddings first of all. You could train a word2vec (see tutorial [here](http://rare-technologies.com/word2vec-tutorial/)) model on some corpus, but in this tutorial we will simply download some pre-trained word2vec embeddings. Download these embeddings [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit).\n",
    "\n",
    "Let's take some sentences to compute the distance between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "\n",
    "sentence1 = 'Obama speaks to the media in Illinois'\n",
    "sentence2 = 'The president greets the press in Chicago'\n",
    "sentence1 = sentence1.lower().split()\n",
    "sentence2 = sentence2.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sentences have very similar content, and as such the WMD should be low. Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.), as these do not contribute a lot to the information in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/olavur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "\n",
    "# Remove stopwords.\n",
    "stop_words = stopwords.words('english')\n",
    "sentence1 = [w for w in sentence1 if w not in stop_words]\n",
    "sentence2 = [w for w in sentence2 if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as mentioned earlier, we will be using some downloaded pre-trained embeddings. We load these into a Gensim Word2Vec model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load_word2vec_format('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's compute the WMD using the `wmdistance` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01746462593\n"
     ]
    }
   ],
   "source": [
    "distance = model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36042560915\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Obama speaks to the media in Illinois'\n",
    "sentence2 = 'Oranges are my favorite type of fruit'\n",
    "sentence1 = sentence1.lower().split()\n",
    "sentence2 = sentence2.lower().split()\n",
    "stop_words = stopwords.words('english')\n",
    "sentence1 = [w for w in sentence1 if w not in stop_words]\n",
    "sentence2 = [w for w in sentence2 if w not in stop_words]\n",
    "\n",
    "distance = model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "print distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
