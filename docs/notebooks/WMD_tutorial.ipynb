{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Mover's Distance\n",
    "\n",
    "Word Mover's Distance (WMD) is a new method that seeks to find the distance between two sets of words (documents, sentences, etc.). The method uses word embeddings via word2vec, and the rest is quite intuitive: match the closest words and sum the distances between them. Another way of saying it is that we find the \"minimum traveling distance\" from one document to another. \n",
    "\n",
    "WMD is illustrated below for two very similar sentences. The sentences have no words in common (non-trivial ones), but by matching the relevant words, WMD is able to accurately measure the similarity between the two sentences.\n",
    "\n",
    "<img src='https://vene.ro/images/wmd-obama.png' height='700' width='700'>\n",
    "\n",
    "\n",
    "This method comes from the article \"From Word Embeddings To Document Distances\" by Matt Kusner et al. ([link to PDF](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf)). It is inspired by the \"Earth Mover's Distance\", and employs a solver of the \"transportation problem\".\n",
    "\n",
    "This short tutorial shows the use of the `wmdistance` method of the Gensim `Word2Vec` class. Stay tuned for another tutorial in kNN classification using Gensim's [docsim](http://radimrehurek.com/gensim/similarities/docsim.html), when this functionality is implemented!\n",
    "\n",
    "> **Note**:\n",
    ">\n",
    "> If you use this software, please consider citing the following papers:\n",
    ">\n",
    "> Ofir Pele and Michael Werman, \"A linear time histogram metric for improved SIFT matching\".\n",
    "> \n",
    "> Ofir Pele and Michael Werman, \"Fast and robust earth mover's distances\".\n",
    ">\n",
    "> Matt Kusner et al. \"From Word Embeddings To Document Distances\".\n",
    "\n",
    "## Using WMD\n",
    "\n",
    "To use WMD, we need some word embeddings first of all. You could train a word2vec (see tutorial [here](http://rare-technologies.com/word2vec-tutorial/)) model on some corpus, but in this tutorial we will simply download some pre-trained word2vec embeddings. Download these embeddings [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit). Training your own embeddings can be beneficial, but to simplify this tutorial, we will be using pre-trained embeddings.\n",
    "\n",
    "Let's take some sentences to compute the distance between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
    "\n",
    "sentence1 = 'Obama speaks to the media in Illinois'\n",
    "sentence2 = 'The president greets the press in Chicago'\n",
    "sentence1 = sentence1.lower().split()\n",
    "sentence2 = sentence2.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sentences have very similar content, and as such the WMD should be low. Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.), as these do not contribute a lot to the information in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/olavur/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "\n",
    "# Remove stopwords.\n",
    "stop_words = stopwords.words('english')\n",
    "sentence1 = [w for w in sentence1 if w not in stop_words]\n",
    "sentence2 = [w for w in sentence2 if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as mentioned earlier, we will be using some downloaded pre-trained embeddings. We load these into a Gensim Word2Vec model class. Note that the embeddings we have chosen here require a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load_word2vec_format('/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's compute the WMD using the `wmdistance` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01746462593\n"
     ]
    }
   ],
   "source": [
    "distance = model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36042560915\n"
     ]
    }
   ],
   "source": [
    "sentence1 = 'Obama speaks to the media in Illinois'\n",
    "sentence2 = 'Oranges are my favorite type of fruit'\n",
    "sentence1 = sentence1.lower().split()\n",
    "sentence2 = sentence2.lower().split()\n",
    "stop_words = stopwords.words('english')\n",
    "sentence1 = [w for w in sentence1 if w not in stop_words]\n",
    "sentence2 = [w for w in sentence2 if w not in stop_words]\n",
    "\n",
    "distance = model.wmdistance(sentence1, sentence2)\n",
    "\n",
    "print distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity queries\n",
    "\n",
    "You can use WMD to get the most similar documents to a query, using the `WmdSimilarity` class. Its interface is similar to what is described in the [Similarity Queries](https://radimrehurek.com/gensim/tut3.html) Gensim tutorial.\n",
    "\n",
    "> **Important note:**\n",
    ">\n",
    "> WMD is a measure of *distance*, which is in fact the opposite of similarity. The similarities in `WmdSimilarity` are simply the *negative distance*. Be careful not to confuse distances and similarities.\n",
    "\n",
    "Let's try similarity queries using some real world data. For that we'll be using Yelp reviews, available at http://www.yelp.com/dataset_challenge. This time around, we are going to train the Word2Vec embeddings on the data ourselves.\n",
    "\n",
    "Below a JSON file with Yelp reviews is read line by line, the text is extracted, tokenized, and stopwords and punctuation are removed. We use the restaurant that has the most reviews to run our queries against, and train Word2Vec on the top 6 restaurants in terms of number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/olavur/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk import word_tokenize\n",
    "download('punkt')  # Download data for tokenizer.\n",
    "\n",
    "# Business IDs of some restaurants.\n",
    "ids = ['4bEjOyTaDG24SY5TxsaUNQ', '2e2e7WgqU1BnpxmQL5jbfw', 'zt1TpTuJ6y9n551sw9TaEg',\n",
    "      'Xhg93cMdemu5pAMkDoEdtQ', 'sIyHTizqAiGu12XMLX3N3g', 'YNQgak-ZLtYJQxlDwN-qIg']\n",
    "\n",
    "# Load some text data.\n",
    "w2v_corpus = []  # Documents to train word2vec on.\n",
    "wmd_corpus = []  # Documents to run queries against.\n",
    "documents = []  # wmd_corpus, with no pre-processing.\n",
    "with open('/home/olavur/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json') as data_file:\n",
    "    for line in data_file:\n",
    "        json_line = json.loads(line)\n",
    "        \n",
    "        if json_line['business_id'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        # Pre-process document.\n",
    "        text = json_line['text'].lower()  # Lower the text.\n",
    "        text = word_tokenize(text)  # Split into words.\n",
    "        text = [w for w in text if not w in stop_words]  # Remove stopwords.\n",
    "        text = [w for w in text if w.isalpha()]  # Remove numbers and punctuation.\n",
    "        \n",
    "        # Add to corpus for training Word2Vec.\n",
    "        w2v_corpus.append(text)\n",
    "        \n",
    "        if json_line['business_id'] == ids[0]:\n",
    "            # Only use this restaurant for corpus for WmdSimilarity.\n",
    "            wmd_corpus.append(text)\n",
    "            documents.append(json_line['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Word2Vec.\n",
    "model = Word2Vec(w2v_corpus, workers=7, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to initialize the similarity class with a corpus and a word2vec model (which provides the embeddings and the `wmdistance` method itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.similarities import WmdSimilarity\n",
    "instance = WmdSimilarity(wmd_corpus, model, num_best=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `num_best` parameter decides how many results the queries return. Now let's try making a query. The output is a list of indeces and similarities of documents in the corpus, sorted by similarity. Note that the output format is slightly different when `num_best` is `None` (i.e. not assigned)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = 'Very good, you should seat outdoor.'\n",
    "\n",
    "# Pre-process query.\n",
    "query = sent.lower()  # Lower the text.\n",
    "query = word_tokenize(query)  # Split into words.\n",
    "query = [w for w in query if not w in stop_words]  # Remove stopwords.\n",
    "query = [w for w in query if w.isalpha()]  # Remove numbers and punctuation.\n",
    "\n",
    "sims = instance[query]  # A query is simply a \"look-up\" in the similarity class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query and the most similar documents, together with the similarities, are printed below. We see that the retrieved documents use the word \"outside\", while the query uses the word \"outdoor\", and these documents are close because these words have the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Very good, you should seat outdoor.\n",
      "\n",
      "-0.750829606837\n",
      "It was good I like the outside\n",
      "\n",
      "-0.755294813249\n",
      "It's a great place if you can sit outside in good weather.\n",
      "\n",
      "-0.852301178568\n",
      "Sat outside under heat lamps.  Good service and good food.  Wonderful place\n"
     ]
    }
   ],
   "source": [
    "print 'Query:'\n",
    "print sent\n",
    "for i in range(3):\n",
    "    print\n",
    "    print sims[i][1]\n",
    "    print documents[sims[i][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "I felt that the prices were extremely reasonable for the Strip\n",
      "\n",
      "-0.710589861037\n",
      "Reasonable prices. Makes for a nice dinner out in the town.\n",
      "\n",
      "-0.831299321403\n",
      "Exceptional food at reasonable prices.  Reservations are a must.\n",
      "\n",
      "-0.858231103992\n",
      "Had lunch here, food price was very reasonable for vegas and the atmosphere was great.\n"
     ]
    }
   ],
   "source": [
    "sent = 'I felt that the prices were extremely reasonable for the Strip'\n",
    "\n",
    "# Pre-process query.\n",
    "query = sent.lower()  # Lower the text.\n",
    "query = word_tokenize(query)  # Split into words.\n",
    "query = [w for w in query if not w in stop_words]  # Remove stopwords.\n",
    "query = [w for w in query if w.isalpha()]  # Remove numbers and punctuation.\n",
    "\n",
    "sims = instance[query]  # A query is simply a \"look-up\" in the similarity class.\n",
    "\n",
    "print 'Query:'\n",
    "print sent\n",
    "for i in range(3):\n",
    "    print\n",
    "    print sims[i][1]\n",
    "    print documents[sims[i][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around, the results are more straight forward; the retrieved documents basically contain the same words as the query. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
